# ğŸ§  Spatio-Temporal Activity Recognition via ResNet-BiLSTM and MHSA-Based Transformer Encoders

This project presents a deep learning pipeline for video classification, focusing on **multi-class activity recognition** using **spatio-temporal modeling**. We integrate **ResNet-18** for spatial encoding, **Bidirectional LSTM** for temporal dynamics, and **Multi-Head Self-Attention (MHSA)** with **Transformer Encoder Layers (TEL)** to capture global dependencies across time.

## ğŸ“Œ Key Highlights

- ğŸ§  **Hybrid Deep Architecture**: Combines CNN-based spatial extraction, RNN-based sequence modeling, and attention-based context modeling.
- ğŸï¸ **Custom Video Dataset Handling**: Uses OpenCV for efficient frame sampling, normalization, and augmentation.
- âš™ï¸ **Training Stability**: Implements gradient clipping, stratified sampling, learning rate scheduling, and early stopping.
- ğŸ·ï¸ **Class Imbalance Aware**: Dynamic class weighting during loss computation for skewed label distributions.
- ğŸ“Š **Rich Evaluation**: Classification report and confusion matrix visualization for validation insights.

---

## Results
![0 86](https://github.com/user-attachments/assets/7a921c0e-952d-42eb-9a18-6d7ba79c0a3a)


## ğŸ“ Architecture Overview

```text
           Input Video
               â†“
         OpenCV Frame Sampling
               â†“
          ResNet-18 Backbone
               â†“
        BiLSTM Temporal Encoder
               â†“
       Multi-Head Self-Attention
               â†“
       Transformer Encoder Layers
               â†“
       Global Average Pooling
               â†“
          Classification Head
               â†“
           Output Class
